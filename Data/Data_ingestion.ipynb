{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IoT Data Ingestion\n",
        "Load JSON files from Volume into Delta Table\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üìã DATA CONFIGURATION VARIABLES\n",
            "================================================================================\n",
            "Project Name:        Telecom_IoT_Dashboard\n",
            "Databricks User:     kunal.gaurav@databricks.com\n",
            "Clean Username:      kunal-gaurav\n",
            "Catalog:             kunal\n",
            "Schema:              telcom\n",
            "Volume Name:         raw_data\n",
            "Volume Path:         /Volumes/kunal/telcom/raw_data\n",
            "Table Name:          kunal.telcom.iot_data_synched_cont\n",
            "\n",
            "================================================================================\n",
            "üóÑÔ∏è  LAKEBASE CONFIGURATION VARIABLES\n",
            "================================================================================\n",
            "Instance Name:       kunal-gaurav-lakebase-instance\n",
            "Instance Capacity:   CU_1\n",
            "Lakebase Catalog:    pg_lakebase_kunal-gaurav\n",
            "Synced Table:        kunal.telcom.iot_data_synced\n",
            "PG Table:            kunal.telcom.iot_metadata\n",
            "================================================================================\n",
            "‚úÖ Catalog 'kunal' ready\n",
            "‚úÖ Schema 'kunal.telcom' ready\n",
            "‚úÖ Volume '/Volumes/kunal/telcom/raw_data' ready\n",
            "‚úÖ Table 'kunal.telcom.iot_data_synched_cont' ready\n"
          ]
        }
      ],
      "source": [
        "%run ./data_store_variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/Volumes/kunal/telcom/raw_data/IOT_LOGS\n",
            "‚úÖ Streaming started: iot_data_streaming\n",
            "üìä Query ID: e6a380b6-8147-4717-9871-a614e82cf44e\n"
          ]
        }
      ],
      "source": [
        "from pyspark.sql.functions import col, from_json\n",
        "from pyspark.sql.types import StructType, StringType, DoubleType, IntegerType, TimestampType\n",
        "\n",
        "schema = StructType() \\\n",
        "    .add(\"tower_id\", StringType()) \\\n",
        "    .add(\"region\", StringType()) \\\n",
        "    .add(\"timestamp\", TimestampType()) \\\n",
        "    .add(\"data_usage_mb\", DoubleType()) \\\n",
        "    .add(\"call_drop_rate\", DoubleType()) \\\n",
        "    .add(\"active_users\", IntegerType())\n",
        "\n",
        "# Volume path for IOT logs\n",
        "volume_path = f\"{VOLUME_PATH}/IOT_LOGS\"\n",
        "print(volume_path)\n",
        "# Checkpoint location\n",
        "checkpoint_path = f\"{VOLUME_PATH}/checkpoint\"\n",
        "\n",
        "# Read from JSON files (simulated streaming)\n",
        "stream_df = (spark.readStream\n",
        "    .format(\"cloudFiles\")\n",
        "    .schema(schema)\n",
        "    .option(\"cloudFiles.format\", \"json\")\n",
        "    .load(volume_path))\n",
        "\n",
        "query = (stream_df\n",
        " .writeStream\n",
        " .format(\"delta\")\n",
        " .outputMode(\"append\")\n",
        " .option(\"checkpointLocation\", checkpoint_path)\n",
        " .trigger(processingTime=\"10 seconds\")\n",
        " .queryName(\"iot_data_streaming\")  # Named query\n",
        " .table(TABLE_NAME)\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Streaming started: {query.name}\")\n",
        "print(f\"üìä Query ID: {query.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kunal.gaurav/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2181: UserWarning: Spark Connect Session expired on the server. Please generate a new session by detaching and reattaching the compute if in a Databricks notebook or job or by calling DatabricksSession.builder.getOrCreate() if using Databricks Connect.\n",
            "  warnings.warn(\n",
            "/Users/kunal.gaurav/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:253: UserWarning: ReleaseExecute failed with exception: <_InactiveRpcError of RPC that terminated with:\n",
            "\tstatus = StatusCode.INTERNAL\n",
            "\tdetails = \"[INVALID_HANDLE.SESSION_CLOSED] The handle bb90dfa4-9904-4d8e-9f65-bd303f9fd489 is invalid. Session was closed. SQLSTATE: HY000\"\n",
            "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\"[INVALID_HANDLE.SESSION_CLOSED] The handle bb90dfa4-9904-4d8e-9f65-bd303f9fd489 is invalid. Session was closed. SQLSTATE: HY000\"}\"\n",
            ">.\n",
            "  warnings.warn(f\"ReleaseExecute failed with exception: {e}.\")\n"
          ]
        },
        {
          "ename": "SparkConnectGrpcException",
          "evalue": "(org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle bb90dfa4-9904-4d8e-9f65-bd303f9fd489 is invalid. Session was closed. SQLSTATE: HY000",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mSparkConnectGrpcException\u001b[39m                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Get query stats\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m query_status = \u001b[43mquery\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstatus\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m60\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müìä STREAMING QUERY STATUS\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/streaming/query.py:105\u001b[39m, in \u001b[36mStreamingQuery.status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    103\u001b[39m \u001b[38;5;129m@property\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstatus\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     proto = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fetch_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    107\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmessage\u001b[39m\u001b[33m\"\u001b[39m: proto.status_message,\n\u001b[32m    108\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33misDataAvailable\u001b[39m\u001b[33m\"\u001b[39m: proto.is_data_available,\n\u001b[32m    109\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33misTriggerActive\u001b[39m\u001b[33m\"\u001b[39m: proto.is_trigger_active,\n\u001b[32m    110\u001b[39m     }\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/streaming/query.py:198\u001b[39m, in \u001b[36mStreamingQuery._fetch_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    196\u001b[39m cmd = pb2.StreamingQueryCommand()\n\u001b[32m    197\u001b[39m cmd.status = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_streaming_query_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m)\u001b[49m.status\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/streaming/query.py:207\u001b[39m, in \u001b[36mStreamingQuery._execute_streaming_query_cmd\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    205\u001b[39m exec_cmd = pb2.Command()\n\u001b[32m    206\u001b[39m exec_cmd.streaming_query_command.CopyFrom(cmd)\n\u001b[32m--> \u001b[39m\u001b[32m207\u001b[39m (_, properties, _) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(pb2.StreamingQueryCommandResult, properties[\u001b[33m\"\u001b[39m\u001b[33mstreaming_query_command_result\u001b[39m\u001b[33m\"\u001b[39m])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1306\u001b[39m, in \u001b[36mSparkConnectClient.execute_command\u001b[39m\u001b[34m(self, command, observations, extra_request_metadata)\u001b[39m\n\u001b[32m   1304\u001b[39m     req.user_context.user_id = \u001b[38;5;28mself\u001b[39m._user_id\n\u001b[32m   1305\u001b[39m req.plan.command.CopyFrom(command)\n\u001b[32m-> \u001b[39m\u001b[32m1306\u001b[39m data, _, metrics, observed_metrics, properties = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1307\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_request_metadata\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[32m   1310\u001b[39m ei = ExecutionInfo(metrics, observed_metrics)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1764\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[39m\n\u001b[32m   1761\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers=\u001b[38;5;28mself\u001b[39m._progress_handlers, operation_id=req.operation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m-> \u001b[39m\u001b[32m1764\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_request_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[32m   1766\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1767\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1768\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1740\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, extra_request_metadata, progress)\u001b[39m\n\u001b[32m   1738\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[32m   1739\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1740\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2056\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   2054\u001b[39m \u001b[38;5;28mself\u001b[39m.thread_local.inside_error_handling = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2055\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m2056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   2058\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCannot invoke RPC\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2137\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   2133\u001b[39m             d.Unpack(info)\n\u001b[32m   2135\u001b[39m             \u001b[38;5;28mself\u001b[39m._handle_rpc_error_with_error_info(info, status.message, status_code)  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2137\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[32m   2138\u001b[39m                 info,\n\u001b[32m   2139\u001b[39m                 status.message,\n\u001b[32m   2140\u001b[39m                 \u001b[38;5;28mself\u001b[39m._fetch_enriched_error(info),\n\u001b[32m   2141\u001b[39m                 \u001b[38;5;28mself\u001b[39m._display_server_stack_trace(),\n\u001b[32m   2142\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n\u001b[32m   2145\u001b[39m         message=status.message,\n\u001b[32m   2146\u001b[39m         sql_state=ErrorCode.CLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n\u001b[32m   2147\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mSparkConnectGrpcException\u001b[39m: (org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle bb90dfa4-9904-4d8e-9f65-bd303f9fd489 is invalid. Session was closed. SQLSTATE: HY000"
          ]
        }
      ],
      "source": [
        "# Get query stats\n",
        "query_status = query.status\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"üìä STREAMING QUERY STATUS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Query Name: {query.name}\")\n",
        "print(f\"Query ID: {query.id}\")\n",
        "print(f\"Is Active: {query.isActive}\")\n",
        "print(f\"Message: {query_status['message']}\")\n",
        "print(f\"Data Available: {query_status['isDataAvailable']}\")\n",
        "print(f\"Trigger Active: {query_status['isTriggerActive']}\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Get recent progress\n",
        "recent_progress = query.recentProgress\n",
        "if recent_progress:\n",
        "    latest = recent_progress[-1]\n",
        "    print(\"\\nüìà LATEST PROGRESS:\")\n",
        "    print(f\"Batch ID: {latest.get('batchId', 'N/A')}\")\n",
        "    print(f\"Input Rows: {latest.get('numInputRows', 0)}\")\n",
        "    print(f\"Processing Time: {latest.get('durationMs', {}).get('triggerExecution', 'N/A')} ms\")\n",
        "    print(f\"Timestamp: {latest.get('timestamp', 'N/A')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kunal.gaurav/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2181: UserWarning: Spark Connect Session expired on the server. Please generate a new session by detaching and reattaching the compute if in a Databricks notebook or job or by calling DatabricksSession.builder.getOrCreate() if using Databricks Connect.\n",
            "  warnings.warn(\n",
            "/Users/kunal.gaurav/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:253: UserWarning: ReleaseExecute failed with exception: <_InactiveRpcError of RPC that terminated with:\n",
            "\tstatus = StatusCode.INTERNAL\n",
            "\tdetails = \"[INVALID_HANDLE.SESSION_CLOSED] The handle c592c8dc-5c09-4c62-879f-d06ae40bb329 is invalid. Session was closed. SQLSTATE: HY000\"\n",
            "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_message:\"[INVALID_HANDLE.SESSION_CLOSED] The handle c592c8dc-5c09-4c62-879f-d06ae40bb329 is invalid. Session was closed. SQLSTATE: HY000\", grpc_status:13}\"\n",
            ">.\n",
            "  warnings.warn(f\"ReleaseExecute failed with exception: {e}.\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Error stopping query: (org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle c592c8dc-5c09-4c62-879f-d06ae40bb329 is invalid. Session was closed. SQLSTATE: HY000\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/kunal.gaurav/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/reattach.py:253: UserWarning: ReleaseExecute failed with exception: <_InactiveRpcError of RPC that terminated with:\n",
            "\tstatus = StatusCode.INTERNAL\n",
            "\tdetails = \"[INVALID_HANDLE.SESSION_CLOSED] The handle c592c8dc-5c09-4c62-879f-d06ae40bb329 is invalid. Session was closed. SQLSTATE: HY000\"\n",
            "\tdebug_error_string = \"UNKNOWN:Error received from peer  {grpc_status:13, grpc_message:\"[INVALID_HANDLE.SESSION_CLOSED] The handle c592c8dc-5c09-4c62-879f-d06ae40bb329 is invalid. Session was closed. SQLSTATE: HY000\"}\"\n",
            ">.\n",
            "  warnings.warn(f\"ReleaseExecute failed with exception: {e}.\")\n"
          ]
        },
        {
          "ename": "SparkConnectGrpcException",
          "evalue": "(org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle c592c8dc-5c09-4c62-879f-d06ae40bb329 is invalid. Session was closed. SQLSTATE: HY000",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mSparkConnectGrpcException\u001b[39m                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 10\u001b[39m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå Error stopping query: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# Verify all streaming queries are stopped\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m active_queries = \u001b[43mspark\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstreams\u001b[49m\u001b[43m.\u001b[49m\u001b[43mactive\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m active_queries:\n\u001b[32m     12\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚ö†Ô∏è  Still \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(active_queries)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m active streaming queries:\u001b[39m\u001b[33m\"\u001b[39m)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/streaming/query.py:223\u001b[39m, in \u001b[36mStreamingQueryManager.active\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    221\u001b[39m cmd = pb2.StreamingQueryManagerCommand()\n\u001b[32m    222\u001b[39m cmd.active = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m queries = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_streaming_query_manager_cmd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m)\u001b[49m.active.active_queries\n\u001b[32m    224\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m [StreamingQuery(\u001b[38;5;28mself\u001b[39m._session, q.id.id, q.id.run_id, q.name) \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m queries]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/streaming/query.py:286\u001b[39m, in \u001b[36mStreamingQueryManager._execute_streaming_query_manager_cmd\u001b[39m\u001b[34m(self, cmd)\u001b[39m\n\u001b[32m    284\u001b[39m exec_cmd = pb2.Command()\n\u001b[32m    285\u001b[39m exec_cmd.streaming_query_manager_command.CopyFrom(cmd)\n\u001b[32m--> \u001b[39m\u001b[32m286\u001b[39m (_, properties, _) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_session\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclient\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexec_cmd\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[32m    288\u001b[39m     pb2.StreamingQueryManagerCommandResult,\n\u001b[32m    289\u001b[39m     properties[\u001b[33m\"\u001b[39m\u001b[33mstreaming_query_manager_command_result\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m    290\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1306\u001b[39m, in \u001b[36mSparkConnectClient.execute_command\u001b[39m\u001b[34m(self, command, observations, extra_request_metadata)\u001b[39m\n\u001b[32m   1304\u001b[39m     req.user_context.user_id = \u001b[38;5;28mself\u001b[39m._user_id\n\u001b[32m   1305\u001b[39m req.plan.command.CopyFrom(command)\n\u001b[32m-> \u001b[39m\u001b[32m1306\u001b[39m data, _, metrics, observed_metrics, properties = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1307\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_request_metadata\u001b[49m\n\u001b[32m   1308\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1309\u001b[39m \u001b[38;5;66;03m# Create a query execution object.\u001b[39;00m\n\u001b[32m   1310\u001b[39m ei = ExecutionInfo(metrics, observed_metrics)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1764\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch\u001b[39m\u001b[34m(self, req, observations, extra_request_metadata, self_destruct)\u001b[39m\n\u001b[32m   1761\u001b[39m properties: Dict[\u001b[38;5;28mstr\u001b[39m, Any] = {}\n\u001b[32m   1763\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Progress(handlers=\u001b[38;5;28mself\u001b[39m._progress_handlers, operation_id=req.operation_id) \u001b[38;5;28;01mas\u001b[39;00m progress:\n\u001b[32m-> \u001b[39m\u001b[32m1764\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_execute_and_fetch_as_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1765\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_request_metadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprogress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress\u001b[49m\n\u001b[32m   1766\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1767\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStructType\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1768\u001b[39m \u001b[43m            \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:1740\u001b[39m, in \u001b[36mSparkConnectClient._execute_and_fetch_as_iterator\u001b[39m\u001b[34m(self, req, observations, extra_request_metadata, progress)\u001b[39m\n\u001b[32m   1738\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m kb\n\u001b[32m   1739\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m-> \u001b[39m\u001b[32m1740\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2056\u001b[39m, in \u001b[36mSparkConnectClient._handle_error\u001b[39m\u001b[34m(self, error)\u001b[39m\n\u001b[32m   2054\u001b[39m \u001b[38;5;28mself\u001b[39m.thread_local.inside_error_handling = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2055\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, grpc.RpcError):\n\u001b[32m-> \u001b[39m\u001b[32m2056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle_rpc_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2057\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(error, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[32m   2058\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCannot invoke RPC\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mclosed\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(error):\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Cursor/Telecom IoT_lakebase_aap_dash/.venv/lib/python3.11/site-packages/pyspark/sql/connect/client/core.py:2137\u001b[39m, in \u001b[36mSparkConnectClient._handle_rpc_error\u001b[39m\u001b[34m(self, rpc_error)\u001b[39m\n\u001b[32m   2133\u001b[39m             d.Unpack(info)\n\u001b[32m   2135\u001b[39m             \u001b[38;5;28mself\u001b[39m._handle_rpc_error_with_error_info(info, status.message, status_code)  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2137\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m convert_exception(\n\u001b[32m   2138\u001b[39m                 info,\n\u001b[32m   2139\u001b[39m                 status.message,\n\u001b[32m   2140\u001b[39m                 \u001b[38;5;28mself\u001b[39m._fetch_enriched_error(info),\n\u001b[32m   2141\u001b[39m                 \u001b[38;5;28mself\u001b[39m._display_server_stack_trace(),\n\u001b[32m   2142\u001b[39m             ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2144\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m SparkConnectGrpcException(\n\u001b[32m   2145\u001b[39m         message=status.message,\n\u001b[32m   2146\u001b[39m         sql_state=ErrorCode.CLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001b[38;5;66;03m# EDGE\u001b[39;00m\n\u001b[32m   2147\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2148\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[31mSparkConnectGrpcException\u001b[39m: (org.apache.spark.SparkSQLException) [INVALID_HANDLE.SESSION_CLOSED] The handle c592c8dc-5c09-4c62-879f-d06ae40bb329 is invalid. Session was closed. SQLSTATE: HY000"
          ]
        }
      ],
      "source": [
        "# Stop the streaming query\n",
        "try:\n",
        "    query.stop()\n",
        "    print(f\"‚èπÔ∏è  Streaming query '{query.name}' stopped successfully\")\n",
        "    print(f\"   Final status: {query.isActive}\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error stopping query: {e}\")\n",
        "\n",
        "# Verify all streaming queries are stopped\n",
        "active_queries = spark.streams.active\n",
        "if active_queries:\n",
        "    print(f\"\\n‚ö†Ô∏è  Still {len(active_queries)} active streaming queries:\")\n",
        "    for q in active_queries:\n",
        "        print(f\"   - {q.name} (ID: {q.id})\")\n",
        "else:\n",
        "    print(\"\\n‚úÖ No active streaming queries\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
