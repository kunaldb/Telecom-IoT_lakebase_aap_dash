{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Store Variables Configuration\n",
        "\n",
        "This notebook contains all configuration variables for the Telecom IoT Dashboard project.\n",
        "\n",
        "All other notebooks should run this notebook using `%run ./data_store_variables` to load these variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Data Configuration Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Project Configuration\n",
        "PROJECT_NAME = \"Telecom_IoT_Dashboard\"\n",
        "\n",
        "# Get Databricks User Information\n",
        "try:\n",
        "    DB_USER = spark.sql(\"SELECT current_user()\").collect()[0][0]\n",
        "    DB_USER_EMAIL = DB_USER\n",
        "    # Extract clean username (before @ or use as-is)\n",
        "    DB_USER_CLEAN = DB_USER.split('@')[0].replace('.', '-').replace('_', '-')\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not get Databricks user (running locally?): {e}\")\n",
        "    DB_USER = \"kunal.gaurav@databricks.com\"\n",
        "    DB_USER_EMAIL = DB_USER\n",
        "    DB_USER_CLEAN = \"kunal-gaurav\"\n",
        "\n",
        "# Unity Catalog Configuration\n",
        "CATALOG_NAME = \"kunal\"\n",
        "SCHEMA_NAME = \"telcom\"\n",
        "\n",
        "# Volume Configuration (parameterized)\n",
        "VOLUME_NAME = \"raw_data\"\n",
        "VOLUME_PATH = f\"/Volumes/{CATALOG_NAME}/{SCHEMA_NAME}/{VOLUME_NAME}\"\n",
        "\n",
        "# Delta Table Configuration\n",
        "TABLE_NAM = \"iot_data_synched_cont\"\n",
        "TABLE_NAME = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAM}\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Lakebase Configuration Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Lakebase Database Instance Configuration\n",
        "INSTANCE_NAME = f\"{DB_USER_CLEAN}-lakebase-instance\"  # DNS compliant\n",
        "INSTANCE_CAPACITY = \"CU_1\"  # Options: CU_1, CU_2, CU_4, CU_8 (Compute Units)\n",
        "\n",
        "# Lakebase Unity Catalog Name (maps to PostgreSQL database)\n",
        "LAKEBASE_CATALOG_NAME = f\"pg_lakebase_{DB_USER_CLEAN}\"\n",
        "\n",
        "# Synced Table (syncs from Delta to PostgreSQL)\n",
        "SYNCED_TABLE_NAME = f\"{CATALOG_NAME}.{SCHEMA_NAME}.iot_data_synced\"\n",
        "\n",
        "# PostgreSQL Table in Lakebase\n",
        "PG_TABLE_NAME = f\"{CATALOG_NAME}.{SCHEMA_NAME}.iot_metadata\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Verification - Display All Variables\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üìã DATA CONFIGURATION VARIABLES\n",
            "================================================================================\n",
            "Project Name:        Telecom_IoT_Dashboard\n",
            "Databricks User:     kunal.gaurav@databricks.com\n",
            "Clean Username:      kunal-gaurav\n",
            "Catalog:             kunal\n",
            "Schema:              telcom\n",
            "Volume Name:         raw_data\n",
            "Volume Path:         /Volumes/kunal/telcom/raw_data\n",
            "Table Name:          kunal.telcom.iot_data_synched_cont\n",
            "\n",
            "================================================================================\n",
            "üóÑÔ∏è  LAKEBASE CONFIGURATION VARIABLES\n",
            "================================================================================\n",
            "Instance Name:       kunal-gaurav-lakebase-instance\n",
            "Instance Capacity:   CU_1\n",
            "Lakebase Catalog:    pg_lakebase_kunal-gaurav\n",
            "Synced Table:        kunal.telcom.iot_data_synced\n",
            "PG Table:            kunal.telcom.iot_metadata\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\" * 80)\n",
        "print(\"üìã DATA CONFIGURATION VARIABLES\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Project Name:        {PROJECT_NAME}\")\n",
        "print(f\"Databricks User:     {DB_USER_EMAIL}\")\n",
        "print(f\"Clean Username:      {DB_USER_CLEAN}\")\n",
        "print(f\"Catalog:             {CATALOG_NAME}\")\n",
        "print(f\"Schema:              {SCHEMA_NAME}\")\n",
        "print(f\"Volume Name:         {VOLUME_NAME}\")\n",
        "print(f\"Volume Path:         {VOLUME_PATH}\")\n",
        "print(f\"Table Name:          {TABLE_NAME}\")\n",
        "print()\n",
        "print(\"=\" * 80)\n",
        "print(\"üóÑÔ∏è  LAKEBASE CONFIGURATION VARIABLES\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Instance Name:       {INSTANCE_NAME}\")\n",
        "print(f\"Instance Capacity:   {INSTANCE_CAPACITY}\")\n",
        "print(f\"Lakebase Catalog:    {LAKEBASE_CATALOG_NAME}\")\n",
        "print(f\"Synced Table:        {SYNCED_TABLE_NAME}\")\n",
        "print(f\"PG Table:            {PG_TABLE_NAME}\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Databricks Setup - Create Resources\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Catalog 'kunal' ready\n"
          ]
        }
      ],
      "source": [
        "# Create Catalog if it doesn't exist\n",
        "try:\n",
        "    spark.sql(f\"CREATE CATALOG IF NOT EXISTS {CATALOG_NAME}\")\n",
        "    print(f\"‚úÖ Catalog '{CATALOG_NAME}' ready\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Catalog setup (running locally?): {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Schema 'kunal.telcom' ready\n"
          ]
        }
      ],
      "source": [
        "# Create Schema if it doesn't exist\n",
        "try:\n",
        "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\")\n",
        "    print(f\"‚úÖ Schema '{CATALOG_NAME}.{SCHEMA_NAME}' ready\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Schema setup (running locally?): {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Volume '/Volumes/kunal/telcom/raw_data' ready\n"
          ]
        }
      ],
      "source": [
        "# Create Volume if it doesn't exist\n",
        "try:\n",
        "    spark.sql(f\"\"\"\n",
        "        CREATE VOLUME IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}.{VOLUME_NAME}\n",
        "    \"\"\")\n",
        "    print(f\"‚úÖ Volume '{VOLUME_PATH}' ready\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Volume setup (running locally?): {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Table 'kunal.telcom.iot_data_synched_cont' ready\n"
          ]
        }
      ],
      "source": [
        "# Create Delta Table if it doesn't exist\n",
        "try:\n",
        "    spark.sql(f\"\"\"\n",
        "        CREATE TABLE IF NOT EXISTS {TABLE_NAME} (\n",
        "        )\n",
        "        USING DELTA\n",
        "    \"\"\")\n",
        "    print(f\"‚úÖ Table '{TABLE_NAME}' ready\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Table setup (running locally?): {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "DataFrame[]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "spark.sql(f\"\"\"ALTER TABLE  {TABLE_NAME} \n",
        "SET TBLPROPERTIES (\n",
        "  delta.enableChangeDataFeed = true\n",
        ")\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚úÖ Configuration Complete\n",
        "\n",
        "All variables are now set and resources are created (if running on Databricks).\n",
        "\n",
        "Use `%run ./data_store_variables` in other notebooks to load these variables.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.14"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
